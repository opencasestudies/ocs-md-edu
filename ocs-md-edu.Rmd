---
title: OpenCaseStudies - Maryland Education Analysis 
author: Open Case Study Team
output:
  html_document:
    md_extensions: -startnum
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: no
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

```{r,warning = F}
library(kableExtra)
library(tidyverse)
library(dplyr)
```


# Motivation



<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/g5ylQgdisTM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>

<center>
[source: https://www.youtube.com/watch?time_continue=8&v=g5ylQgdisTM]
</center>



# What is the data?


The data files were downloaded from [the Maryland Report Card website](http://reportcard.msde.maryland.gov/Graphs/#/Introduction/Intro/uction/17/uction/17/3). It aims to share the most current information available to help stakeholders understand and measure student achievement in all 24 local school systems. You can find [`Data Download`](http://reportcard.msde.maryland.gov/Graphs/#/DataDownloads/datadownload/3/17/6/99/XXXX) on the bottom part of the web page, which provides public data for download. 

While all this knowledge can be empowering, too much information can be overwhelming. Important details may be lost in a flood of data, and a large amount of information can make it difficult to make decisions. These files are large because they contain information for 1600+ schools in Maryland, based on school level and county level, ranging from 2003 to 2018. Admittedly, county level data are less and tidier, so it is easier to deal with. But we believe it is school level data's 'untidiness' that guarantees a detailed and diverse anaysis. So we decide to focus on 5 files of year 2017, and mainly make use of school level data. 



# Data Import

We saved all files in a folder called 'data2017', when you need to deal with multiple files in a folder, setting environment is necessary since it enables you to find what you want more quickly and accurately.

```{r }
setwd('./data2017')

files = list.files(pattern="*.csv")

data <- lapply(files, function(x){
  read.csv(x, header=T)})

```

'data' is a large list containing 5 files:

  * Cohort_Grad_Rate_2017.csv
  * PARCC_2017.csv
  * Special_Services_2017.csv
  * Student_Mobility_2017.csv
  * Wealth_Expenditures_Data_2017.csv

# Data Wrangling

> Tidy datasets are all alike, but every messy dataset is messy in its own way. - Hadley Wickham

This section includes large amount of data wrangling such as gather, merge, mutate etc. Each step requires a specific tool to complete, and we would like to show the data set again and again everytime we apply a specific tool. RStudio prodives a data viewer that enables you to look inside data frames and other rectangular data structures. You can invoke the viewer in a console by calling the `View()` function on the data frame you want to look at.

## Graduation Rate

Have a qucik look of graduation rate file:

```{r eval = FALSE}
View(data[[1]])
```

![](plots/grad.1.png)

From above, in the original dataset, there are several problems need to be fixed. We would deal with them one by one in later steps.


### selection, rename 

We notice that if 'School.Number' equals to 'A', then this row corresponds to county level data. Also, there are two kinds of graduation rate and we may use 4-years graduation rate since it is more representative. The last step is the selection of necessary variables 

Function `filter()` and `select()` in [package `dplyr`](https://dplyr.tidyverse.org) will help us to finish the first wrangling step.  

```{r}
df_grad <- data[[1]] %>%
 filter( School.Number != 'A', 
        Cohort == '4-year adjusted cohort') %>%
 select(LEA.Name, School.Number,
         School.Name, Grad.Rate)

colnames(df_grad) <- c('county', 'sch.num',
                       'sch', 'grad.rate')
```


![](plots/grad_2.png)

See what we get! Only school level information and useful variables are kept, which is neater. Next, let's try to deal with missing values.


### missing value

There are a lot of missing values in the dataset, which would influence further analysis if we leave them there. As we shown before, missing values exist in different forms:

* `'>= 95.00','<= 5.00'` : The graduation rate is too high or too low.
* `'*'` : This schools' graduation rate is simply not present in the data.

```{r}
summary(df_grad[2:4]) 
```


Pay attention to the first kinds of missing value. We are not able to calculate the exact graduate rate because of the lack of information so we decide to use `gsub()` function to replace incomplete values. `gsub()` function replaces all matches of a string. Elements of string vectors which are not substituted will be returned unchanged.

We would use 2.5 to replace '<= 5.00' and 97.5 to replace '>= 95.00'.

```{r}
df_grad$grad.rate<- gsub('<= 5.00','2.5', df_grad$grad.rate)
df_grad$grad.rate <- gsub('>= 95.00','97.5', df_grad$grad.rate)
```

![](plots/grad_3.png)

The second type of missing value ('*') is confusing, let's extract them out and have a look:

```{r}
head(df_grad[df_grad$grad.rate == '*', ])
```

If we search [Marley Glen School](https://www.aacps.org/domain/1364), we may found it mainly provides program for students with disabilities. Also, [Ruth Parker Eason School](https://www.aacps.org/Page/3684) provides a special education program for students with moderate to severe disabilities. So we guess schools with '*' graduation rate mainly provide education for students with moderate to severe disabilities. We can either delete it or replace them as 'NA'. Here we use the second method.

```{r}
df_grad$grad.rate <- na_if(df_grad$grad.rate, '*')

```


![](plots/grad_4.png)



### unique id

As we mentioned, we need to extract information from several files and merge them together, requiring a key that specifies each observation uniquely. Possible choice in present dataset can be `sch.num` or `sch.name` but we need to check whether each of them appear once in the dataset. If not, they are not suitable choice because of the lack of uniqueness.

```{r}
summary(df_grad[,2:3])
```

From the summary, whether for `sch.num` or `sch.name`, the frequency of some values is larger than 1. Therefore it is definitely important to define a unique element by ourselves. 

```{r}
df_grad <- df_grad %>%
    within( id <- paste(county, sch, sep = '-'))

```

The combination of `county` and `sch` generates a new variable - `id`. To check its uniqueness, we use function `table()`, which builds a contingency table of the counts at each combination of factor levels. The `as.data.frame()` function converts the array-based representation of a contingency table to a data frame containing two variable: each factor `Var1`and its frequency `Freq`.

```{r}
tab <- as.data.frame(table(df_grad$id))
str(tab)
tab[tab$Freq > 1,]
```

Up to now, we get a unique key element `id` and finally create the ideal dataset - a tidy dataset.


![](plots/grad_5.png)

## PARCC

[Partnership for Assessment of Readiness for College and Careers (PARCC)](https://parcc.pearson.com) 

Patcc is a large file with `r nrow(data[[2]] )` observations and `r ncol(data[[2]] )` variables.

### selection, rename 

```{r}
colnames(data[[2]])
```


```{r}
df_parcc <- data[[2]] %>%
  filter( School.Number != 'A' ) %>%
  select(3,4,5,6,9,11,13,15,17)

colnames(df_parcc) <- c('county','sch.num', 
                        'sch', 'subject','L1',
                        'L2','L3','L4','L5')

summary(data[[2]]$Assessment)

```


![](plots/parcc_1.png)

```{r}
df_parcc <- df_parcc %>%
  filter(subject %in% c('English/Language Arts Grade 10','Algebra 1','Algebra 2'))
```

![](plots/parcc_2.png)


### missing value

```{r}
summary(df_parcc[, 5:9])
```


```{r}
df_parcc[,5:9] <- 
  lapply(df_parcc[,5:9], function(x) as.numeric(as.character(x)))

sapply(df_parcc[,5:9], class)
```


![](plots/parcc_3.png)

We only care about the proportion of L4 and L5 but NAs are distributed differently. To simply wrangling steps, we reduce these five levels into two new levels : weak performance (L1, L2 and L3) and excellent performance (L4 and L5). Then deal with NAs in three ways. If values of excellent performance are complete, then `pro` equals to sum of L4 and L5. Such as Algebra 1 assessment of 'Washington Middle', its `pro` should be $85.5 + 9.7 = 95.2$; If values of weak performance exist, `pro` equals to 100 percent minus the sum of L1, L2 and L3. For example, the `pro` of Algebra 1 for 'Fort Hill High' equlas to $100-23.9-44.6-21.7 = 9.8$ ; 

If missing values exist both in these two levels, we will first use 100 percent minus existing values. Then divide the difference to the number of missing value and replace NA with this quotient. For instance - 'Brooklyn Park Middle', there are three NAs, so $NA = (100-20.8-72.7)/3=2.2$, and `pro` equals to $72.7+2.2=74.9$. The reason we don't replace these NAs as 2.5 directly is that there is a restriction we don't want to obey - the sum of 5 levels equals to 100 percent. It is more reasonable to assume missing values are uniformly distributed than replace them with a fixed value directly.


```{r}
df_parcc$pro <- NA

indx1 <- !is.na(df_parcc$L4) &  !is.na(df_parcc$L5)
df_parcc[indx1, 'pro'] <- rowSums(df_parcc[indx1,8:9])

sum(is.na(df_parcc$pro))

indx2 <- !is.na(df_parcc$L1) &  !is.na(df_parcc$L2) & !is.na(df_parcc$L3)
#indx1 <- !is.na(df_parcc[, 5:7])
df_parcc[indx2, 'pro'] <-  100-rowSums(df_parcc[indx2,5:7])

sum(is.na(df_parcc$pro)) 
```



```{r}
indx3 <- is.na(df_parcc$pro)

n <- rowSums(is.na(df_parcc[indx3, 5:9]))
na_sum <- 100-rowSums(df_parcc[indx3, 5:9], na.rm = TRUE)
na <- round(na_sum/n,1)

for (i in 1:203){
  df_parcc[indx3, 5:9][i,
                       which(is.na(df_parcc[indx3, 5:9][i,]))] <- na[i]
}

df_parcc[indx3, 'pro'] <- rowSums(df_parcc[indx3,8:9])
sum(is.na(df_parcc$pro)) 
```




```{r}
pac <- df_parcc %>%
  within( id <- paste(county, sch, sep = '-')) %>%
  select(1,2,3,4,10,11)
```

### extract assessment information

The proportion of high performance in these assessment ($p$) plays an role of response variable in our later analysis. Thus, splicting them out first makes further analysis convenient.

$$ p_{ela} = \beta_0+\beta_{1} x_1+ \beta_{2} x_2 $$

we choose grade 10 since we believe grade 10 is the most representative.

```{r}
df_ela <- pac[grep("English/Language Arts Grade 10", pac$subject), ]
df_alg1 <- pac[grep("Algebra 1", pac$subject), ]
df_alg2 <- pac[grep("Algebra 2", pac$subject), ]
```

## Special Services

```{r}
df_spc <- data[[3]] %>%
  filter( School.Number != 'A', School.Type == 'High' ) %>%
  within( id <- paste(LEA.Name, School.Name, sep = '-')) %>%
  #subset(id %in% pac$id) %>%
  select(3,4,5,9,21) 

colnames(df_spc) <- c('county', 'sch.num', 'sch', 'farms', 'id')

summary(df_spc)
```

![](plots/farms_1.png)


### missing value

```{r}
df_spc[df_spc$farms == '*',]
```



```{r}
df_spc$farms <- gsub('<= 5.0','2.5', df_spc$farms)
df_spc$farms <- na_if(df_spc$farms, '*')
```

![](plots/farms_2.png)


## Student Mobility

```{r}
df_mob <- data[[4]] %>%
  filter( School.Number != 'A', School.Type == 'High' )%>%
  within( id <- paste(LEA.Name, School.Name, sep = '-')) %>%
  select(3,4,5,11,15)

colnames(df_mob) <- c('county', 'sch.num', 'sch', 'withdraw', 'id')

summary(df_mob)
```


![](plots/mob_1.png)

```{r}
df_mob$withdraw <- gsub('<= 5.00','2.5', df_mob$withdraw)
df_mob$withdraw <- gsub('>= 95.00','97.5', df_mob$withdraw)
df_mob$withdraw <- as.numeric(df_mob$withdraw)

summary(df_mob)
```



## Wealth Expenditures

```{r}
df_exp <- data[[5]]
```

![](plots/exp_1.png)

## Final dataset

### example
```{r}
temp1 <- df_ela %>%
  select(6,1,5) %>%
  left_join(df_grad[, c('grad.rate', 'id')], by = 'id') %>%
  left_join( df_mob[, c('withdraw', 'id')], by = 'id') %>%
  left_join( df_spc[, c('farms', 'id')], by = 'id')
```

```{r}
L_ela <- list(df_ela[, c('id', 'county', 'pro')], df_grad[, c('grad.rate', 'id')], 
              df_mob[, c('withdraw', 'id')], df_spc[, c('farms', 'id')])

temp2 <- Reduce(function(x, y) merge(x, y, by = 'id'), L_ela)
temp3 <- Reduce(function(x,y) left_join(x, y, by = 'id'), L_ela)

all(temp1$id== temp3$id)
```

### apply function

```{r}
func_merge <- function(data){
  L <- list(data[, c('id', 'county', 'pro')], df_grad[, c('grad.rate', 'id')], 
              df_mob[, c('withdraw', 'id')], df_spc[, c('farms', 'id')])
  final_data <- Reduce(function(x,y) left_join(x, y, by = 'id'), L)
  return(final_data)
}


df_ELA <- func_merge(df_ela)
df_ALG1 <- func_merge(df_alg1)
df_ALG2 <- func_merge(df_alg2)

```